{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI assisted Chatbot\n",
    "\n",
    "## Saving Bill Data\n",
    "\n",
    "```mermaid \n",
    "sequenceDiagram\n",
    "    participant C as CongressAPI\n",
    "    participant PT as Pipeline Transforms\n",
    "    box \"Storage\"\n",
    "        participant S3 as S3 Storage\n",
    "        participant PG as PostgreSQL\n",
    "        participant M as Milvus\n",
    "    end\n",
    "\n",
    "    C->>PT: Bill Comes In\n",
    "    PT->>S3: BillPDF\n",
    "    PT->>M: Chunk Bill Text, Generate Embeddings\n",
    "    PT->>S3: BillPDF.embeddings\n",
    "    PT->>PG: Create Bill and BillChunk Entities with Embedding IDs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Querying Bill Text\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as User\n",
    "    participant PT as Pipeline Transforms\n",
    "    participant M as Milvus\n",
    "    participant DB as Postgres\n",
    "\n",
    "    U->>PT: Submit Query\n",
    "    PT->>M: Generate Embedding for Query\n",
    "    M->>M: Perform Similarity Search\n",
    "    M->>PT: Return Closest Embedding IDs\n",
    "    DB->>PT: Retrieve BillChunks by embedding ID\n",
    "    PT->>U: Return Relevant Chunks and Optional Full Bill\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install PyPDF2 requests numpy pymilvus prisma pydantic boto3 minio langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/djdaniels/code/active_workspace/jupyter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "cwd = os.getcwd()\n",
    "repo_root = os.path.abspath(os.path.join(cwd, \"../../../\"))\n",
    "sys.path.append(repo_root)\n",
    "print(repo_root)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 11:10:19,324 - INFO - Cache loaded from ./cache/embeddings_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from pymilvus import CollectionSchema, FieldSchema, DataType, Collection, connections\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import pickle\n",
    "from utils.request_cache import RequestCache\n",
    "from utils.s3_connector import S3Connector\n",
    "\n",
    "request_cache = RequestCache(\"./cache/embeddings_cache.pkl\")\n",
    "\n",
    "api_url = \"https://api.openai.com/v1/embeddings\"\n",
    "openai_api_key = os.environ.get(\"CHATGPT_API_KEY\")\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "}\n",
    "\n",
    "\n",
    "test_file_path = \"./public-law-101-336.pdf\"\n",
    "bill_name = os.path.splitext(os.path.basename(test_file_path))[0]\n",
    "pdf_file_key = f\"{bill_name}/{os.path.basename(test_file_path)}\"\n",
    "# Example Usage\n",
    "# s3_connector = S3Connector(\"bills\")\n",
    "# s3_connector.put_file(test_file_path, pdf_file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    logging.info(f\"Extracting text from {pdf_path}\")\n",
    "    start_time = time.time()\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\".join(\n",
    "            reader.pages[page_num].extract_text()\n",
    "            for page_num in range(len(reader.pages))\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"Text extraction completed {end_time - start_time:.2f}\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_embeddings_for_chunks(chunks, request_cache):\n",
    "    all_embeddings = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_hash = RequestCache.generate_key(chunk)\n",
    "\n",
    "        embeddings = request_cache.get(chunk_hash)\n",
    "        if embeddings is None:\n",
    "            start_time = time.time()\n",
    "            data = {\"model\": \"text-similarity-davinci-001\", \"input\": chunk}\n",
    "            response = requests.post(url=api_url, headers=headers, json=data)\n",
    "            end_time = time.time()\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    embeddings = response.json()[\"data\"][0][\"embedding\"]\n",
    "                    request_cache.set(chunk_hash, embeddings)\n",
    "                except KeyError as e:\n",
    "                    logging.error(f\"KeyError: {e} in response: {response.json()}\")\n",
    "                    continue\n",
    "            else:\n",
    "                logging.error(\n",
    "                    f\"Error in API request: {response.status_code} - {response.text}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            logging.info(\n",
    "                f\"Generated embeddings for chunk {i+1} in {end_time - start_time:.2f} seconds\"\n",
    "            )\n",
    "\n",
    "        all_embeddings.append(embeddings)\n",
    "    return all_embeddings\n",
    "\n",
    "\n",
    "def chunk_text_by_tokens(text, max_tokens=2047, overlap=0):\n",
    "    logging.info(\"Chunking text by tokens\")\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    token_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        estimated_tokens = (\n",
    "            len(word) // 4 + 1\n",
    "        )  # Rough estimation of tokens for each word\n",
    "        token_count += estimated_tokens\n",
    "\n",
    "        if token_count < max_tokens:\n",
    "            current_chunk.append(word)\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            token_count = estimated_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    logging.info(f\"Text chunked into {len(chunks)} parts based on tokens\")\n",
    "    return chunks\n",
    "\n",
    "def process_bill_pdf(bill_pdf_path, request_cache):\n",
    "    # sourcery skip: inline-immediately-returned-variable\n",
    "    bill_text = extract_text_from_pdf(bill_pdf_path)\n",
    "    chunks = chunk_text_by_tokens(bill_text)\n",
    "    embeddings = get_embeddings_for_chunks(chunks, request_cache)\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Run the procedure to generate embeddings of the text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 11:11:58,367 - INFO - Extracting text from ./public-law-101-336.pdf\n",
      "2023-11-27 11:11:58,888 - INFO - Text extraction completed 0.52\n",
      "2023-11-27 11:11:58,888 - INFO - Chunking text by tokens\n",
      "2023-11-27 11:11:58,891 - INFO - Text chunked into 23 parts based on tokens\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/djdaniels/code/active_workspace/jupyter/notebooks/political_analysis/congress_gov/embeddings.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/djdaniels/code/active_workspace/jupyter/notebooks/political_analysis/congress_gov/embeddings.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text_chunks \u001b[39m=\u001b[39m chunk_text_by_tokens(pdf_text, overlap\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/djdaniels/code/active_workspace/jupyter/notebooks/political_analysis/congress_gov/embeddings.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m embeddings \u001b[39m=\u001b[39m get_embeddings_for_chunks(text_chunks, request_cache)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/djdaniels/code/active_workspace/jupyter/notebooks/political_analysis/congress_gov/embeddings.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvector dimensionality is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39;49m(embeddings[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/djdaniels/code/active_workspace/jupyter/notebooks/political_analysis/congress_gov/embeddings.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Save cache\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/djdaniels/code/active_workspace/jupyter/notebooks/political_analysis/congress_gov/embeddings.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m request_cache\u001b[39m.\u001b[39msave()\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "pdf_text = extract_text_from_pdf(test_file_path)\n",
    "text_chunks = chunk_text_by_tokens(pdf_text, overlap=100)\n",
    "embeddings = get_embeddings_for_chunks(text_chunks, request_cache)\n",
    "logging.info(f\"vector dimensionality is {len(embeddings[0])}\")\n",
    "\n",
    "# Save cache\n",
    "request_cache.save()\n",
    "\n",
    "concatenated_embeddings = np.concatenate(embeddings)\n",
    "\n",
    "pickle.dump(\n",
    "    concatenated_embeddings, open(\"../data/public-law-101-336.embeddings.pkl\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Define the fields in your collection\n",
    "# Assuming each embedding is a 256-dimensional vector\n",
    "embedding_field = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=256)\n",
    "\n",
    "# Create a primary key field if you want to uniquely identify each embedding\n",
    "id_field = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True)\n",
    "\n",
    "# Define the schema of the collection\n",
    "schema = CollectionSchema(\n",
    "    fields=[id_field, embedding_field], description=\"Bill Embeddings Collection\"\n",
    ")\n",
    "\n",
    "# Name of the collection\n",
    "collection_name = \"BillEmbeddings\"\n",
    "\n",
    "# Create the collection in Milvus\n",
    "\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "# Assuming you have a list of embeddings and their corresponding IDs\n",
    "embeddings = [concatenated_embeddings]  # Your embeddings\n",
    "ids = [1]  # Integer IDs corresponding to each embedding\n",
    "\n",
    "# Insert data into the collection\n",
    "mr = collection.insert([ids, embeddings])\n",
    "\n",
    "# Optionally, you can flush the collection to make data searchable\n",
    "collection.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-byZSjI24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
